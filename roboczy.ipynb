{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2179bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "import io\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283287a",
   "metadata": {},
   "source": [
    "Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7924976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- KONFIGURACJA ---\n",
    "data_dir = Path('./data')\n",
    "csv_paths = sorted(data_dir.glob('*.csv'))\n",
    "\n",
    "# Regex do filtrowania (bez zmian)\n",
    "mies_regex = re.compile(r'MIES', re.IGNORECASE)\n",
    "column_filter = lambda col: not mies_regex.search(col)\n",
    "\n",
    "# --- POPRAWIONA FUNKCJA WCZYTUJĄCA ---\n",
    "def try_read_csv(path, col_filter=None):\n",
    "    # ELA zazwyczaj używa utf-8 lub cp1250, a separatorem jest średnik.\n",
    "    # Zmieniam kolejność: najpierw średnik, potem reszta.\n",
    "    encodings = ['utf-8', 'utf-8-sig', 'cp1250', 'latin-1']\n",
    "    seps = [';', '\\t', ',', '|']\n",
    "    \n",
    "    read_kwargs = {'usecols': col_filter} if col_filter else {}\n",
    "    last_exc = None\n",
    "\n",
    "    for enc in encodings:\n",
    "        for sep in seps:\n",
    "            try:\n",
    "                # 1. Próba silnikiem C (szybki)\n",
    "                df = pd.read_csv(\n",
    "                    path, \n",
    "                    encoding=enc, \n",
    "                    sep=sep, \n",
    "                    engine='c', \n",
    "                    low_memory=False, \n",
    "                    on_bad_lines='skip',\n",
    "                    **read_kwargs\n",
    "                )\n",
    "                \n",
    "                # --- SANITY CHECK (NOWOŚĆ) ---\n",
    "                # Jeśli df jest pusty LUB ma mniej niż 2 kolumny -> to prawdopodobnie zły separator\n",
    "                if df.empty or df.shape[1] < 2:\n",
    "                    raise ValueError(f\"Suspicious shape {df.shape} with sep='{sep}'\")\n",
    "\n",
    "                return df, f\"{enc}, sep='{sep}', engine=c\"\n",
    "\n",
    "            except Exception as e_c:\n",
    "                # To nie jest właściwy separator/kodowanie, idziemy dalej\n",
    "                pass\n",
    "\n",
    "        # 2. Fallback: Silnik Python z autodetekcją (wolniejszy, inteligentniejszy)\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                path, \n",
    "                encoding=enc, \n",
    "                sep=None,  # Autodetekcja\n",
    "                engine='python', \n",
    "                on_bad_lines='skip',\n",
    "                **read_kwargs\n",
    "            )\n",
    "            \n",
    "            if df.empty or df.shape[1] < 2:\n",
    "                 raise ValueError(f\"Suspicious shape {df.shape} with auto-sep\")\n",
    "                 \n",
    "            return df, f\"{enc}, sep=auto, engine=python\"\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3. Ostatnia deska ratunku: wczytanie jako tekst i wymuszenie\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            raw = f.read()\n",
    "        \n",
    "        text = raw.decode('utf-8', errors='replace')\n",
    "        # Próba wymuszenia średnika na \"brudnym\" tekście\n",
    "        df = pd.read_csv(\n",
    "            io.StringIO(text), \n",
    "            sep=';', \n",
    "            engine='python', \n",
    "            on_bad_lines='skip',\n",
    "            **read_kwargs\n",
    "        )\n",
    "        return df, \"decoded bytes, forced sep=';'\"\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"CRITICAL: Could not read file. Last error: {e}\")\n",
    "\n",
    "# --- PĘTLA ŁADUJĄCA ---\n",
    "loaded = []\n",
    "print(\"Rozpoczynam wczytywanie plików z walidacją kształtu danych...\")\n",
    "\n",
    "for p in csv_paths:\n",
    "    try:\n",
    "        df, meta = try_read_csv(p, col_filter=column_filter)\n",
    "        print(f\"✅ Read {p.name:<40} | {meta:<30} | shape: {df.shape}\")\n",
    "        loaded.append((p.stem, df))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed {p.name}: {e}\")\n",
    "\n",
    "dataframes = [df for _, df in loaded]\n",
    "dataframes_dict = dict(loaded) # Zmieniam nazwę na czystszą"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a152727",
   "metadata": {},
   "source": [
    "Czyszczenie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykrywanie kolumn datowych\n",
    "def _looks_like_date_series(s: pd.Series, sample_size: int = 200, threshold: float = 0.5) -> bool:\n",
    "    \"\"\"\n",
    "    Sprawdza czy seria (kolumna) wygląda jak data.\n",
    "    Szuka wzorców takich jak: 2020-01-15, 01/12/2020, oraz nazw miesięcy\n",
    "    \"\"\"\n",
    "    # Konwertuje do tekstu i usuwa puste wartości\n",
    "    vals = s.dropna().astype(str)\n",
    "    if vals.empty:\n",
    "        return False\n",
    "    \n",
    "    # Bierze losową próbkę (max 200 wartości) do analizy\n",
    "    vals = vals.sample(min(len(vals), sample_size), random_state=0)\n",
    "    \n",
    "    # Regex dla dat: szuka liczb rozdzielonych myślnikami/kreskami/spacjami\n",
    "    # np. 2020-01-15 lub 15/12/2020\n",
    "    date_regex = re.compile(r'^\\s*\\d{1,4}(?:[\\-\\/\\.\\s]\\d{1,2}){1,2}\\s*$')\n",
    "    \n",
    "    # Regex dla nazw miesięcy (polska + angielska)\n",
    "    month_names = re.compile(\n",
    "        r'\\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|'\n",
    "        r'styc|lut|mar|kwi|maj|cze|lip|sie|wrz|paź|lis|gru)\\b', \n",
    "        re.I\n",
    "    )\n",
    "    \n",
    "    # Sprawdza czy wartości pasują do wzoru daty LUB zawierają nazwę miesiąca\n",
    "    matches = vals.str.match(date_regex) | vals.str.contains(month_names)\n",
    "    \n",
    "    # Jeśli co najmniej 50% wartości to daty, zwróć True\n",
    "    return matches.sum() >= max(1, int(threshold * len(vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a1914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Czyszczenie dataframe'u\n",
    "def clean_df(df: pd.DataFrame, name: str = \"\", remove_outliers: bool = True) -> tuple:\n",
    "    \"\"\"\n",
    "    Czyści ramkę danych i (opcjonalnie) usuwa outliery.\n",
    "    Zwraca: (df_po_czyszczeniu, df_przed_usunieciem_outlierow)\n",
    "    (Blok usuwania outlierów został zakomentowany.)\n",
    "    \"\"\"\n",
    "    # Robimy kopię, żeby nie modyfikować oryginalnego obiektu przekazanego do funkcji.\n",
    "    df = df.copy()\n",
    "\n",
    "    # Lista kolumn, które chcemy POMINAĆ przy wykrywaniu outlierów (np. ID, nazwy)\n",
    "    skip_columns_for_outliers = {\n",
    "        'p_rok_od', 'p_kierunek_id', 'p_poziom', 'p_forma', 'p_uczelnia_id',\n",
    "        'p_nazwa_uczelni', 'p_jedn', 'p_nazwa_jedn', 'p_woj', 'p_profil',\n",
    "        'p_dziedzina_new', 'p_uczelnia_skrot', 'p_poziom_tekst_pl',\n",
    "        'p_nazwa_kierunku_pelna', 'p_kierunek_nazwa', 'p_spec_nazwa', 'u_uczelnia_id', \"u_n\", \"u_n_wzus\",\"u_n_pozazus\", \"u_proc_wzus\",\"u_proc_pozazus\",\"u_n_dosw_rekr\", \"p_n\",\n",
    "        \"u_n_dosw_studia\", \"p_n_wzus\",\n",
    "    }\n",
    "\n",
    "    # KROK 1: Normalizuje nazwy kolumn\n",
    "    # Zamienia na małe litery, usuwa spacje, zastępuje je podkreśleniami\n",
    "    df.columns = [str(c).strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    # Usuwamy ewentualne duplikaty nazw kolumn (może się zdarzyć przy złym imporcie).\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # KROK 2: Usuwa całkowicie puste kolumny\n",
    "    # axis=1 oznacza kolumny, how=\"all\" oznacza całkowicie puste\n",
    "    df.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "    # KROK 3: Naprawia tekstowe kolumny\n",
    "    # Bierze tylko kolumny typu tekstowego (object)\n",
    "    obj_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "    for c in obj_cols:\n",
    "        # Konwertuj do string type, usuń spacje na początku/końcu, usuń znaki BOM\n",
    "        df[c] = df[c].astype(\"string\").str.strip().str.replace(\"\\ufeff\", \"\", regex=False)\n",
    "\n",
    "         # Zamienia typowe znaczniki braku danych na pd.NA (None)\n",
    "    df.replace({\"\": pd.NA, \"NA\": pd.NA, \"N/A\": pd.NA, \"na\": pd.NA, \"-\": pd.NA, \"—\": pd.NA, \"None\": pd.NA}, inplace=True)\n",
    "\n",
    "    # KROK 4: Konwertuje kolumny liczbowe (tekst → liczby)\n",
    "    for c in df.columns:\n",
    "        # Jeśli kolumna jest tekstem, spróbuj skonwertować na liczby\n",
    "        if df[c].dtype == \"object\" or pd.api.types.is_string_dtype(df[c]):\n",
    "            # Usuń spacje wewnątrz liczb\n",
    "            s = df[c].astype(\"string\").str.replace(r\"\\s+\", \"\", regex=True)\n",
    "            # Zamień przecinki na kropki (polski format → międzynarodowy)\n",
    "            s = s.str.replace(\",\", \".\", regex=False)\n",
    "            # Usuń wszystkie znaki oprócz cyfr, kropki i minusa\n",
    "            s_clean = s.str.replace(r\"[^0-9\\.\\-]\", \"\", regex=True)\n",
    "            # Konwertuj na liczby (błędy zamień na NaN)\n",
    "            coerced = pd.to_numeric(s_clean, errors=\"coerce\")\n",
    "            non_null_count = coerced.notna().sum()\n",
    "            # Jeśli co najmniej 30% wartości to liczby (nie puste), zaakceptuj konwersję\n",
    "            if non_null_count > 0 and non_null_count >= max(1, int(0.3 * len(coerced))):\n",
    "                df[c] = coerced\n",
    "\n",
    "    # KROK 5: Konwertuje kolumny datowe (tekst → daty)\n",
    "    for c in df.columns:\n",
    "        # Jeśli kolumna jest tekstem\n",
    "        if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_string_dtype(df[c]):\n",
    "            # Sprawdź czy wygląda jak data\n",
    "            if _looks_like_date_series(df[c]):\n",
    "                # Wyłącz warningi podczas konwersji\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "                    # Konwertuj na datetime, przyjmując format DD/MM/YYYY\n",
    "                    parsed = pd.to_datetime(df[c], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "                    # Jeśli co najmniej 30% wartości zostało skonwertowane, zaakceptuj\n",
    "                if parsed.notna().sum() >= max(1, int(0.3 * len(parsed))):\n",
    "                    df[c] = parsed\n",
    "\n",
    "    # KROK 6: Usuwa puste wiersze i duplikaty\n",
    "    # Usuwa wiersze które są całkowicie puste\n",
    "    df.dropna(axis=0, how=\"all\", inplace=True)\n",
    "    # Usuwa wiersze które są całkowicie identyczne\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Zapisujemy snapshot ramki po standardowym czyszczeniu, przed ewentualnym usuwaniem outlierów.\n",
    "    df_before_outlier_removal = df.copy()\n",
    "\n",
    "    # KROK 7: Usuwa wartości skrajne (outliers) — metoda IQR\n",
    "    # POMIJAMY WYBRANE KOLUMNY (takie jak ID, kody, poziomy, kierunki itd.)\n",
    "    \"\"\"\n",
    "    if remove_outliers:\n",
    "        OUTLIER_QUANTILE = 0.99\n",
    "        numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns.tolist()]\n",
    "\n",
    "        mask_total = pd.Series(False, index=df.index)\n",
    "        removed_entries = []\n",
    "\n",
    "        for c in numeric_cols:\n",
    "            if c in skip_columns_for_outliers:\n",
    "                continue\n",
    "            upper = df[c].quantile(OUTLIER_QUANTILE)\n",
    "            if pd.isna(upper):\n",
    "                continue\n",
    "            mask = df[c].notna() & (df[c] > upper)\n",
    "            if mask.any():\n",
    "                for idx in df.index[mask]:\n",
    "                    removed_entries.append((idx, c, df.at[idx, c]))\n",
    "                mask_total |= mask\n",
    "\n",
    "        if mask_total.any():\n",
    "            total_to_remove = int(mask_total.sum())\n",
    "            print(f\"[outliers - percentyl {OUTLIER_QUANTILE}] {name} :: usuwam {total_to_remove} wierszy zawierających TOP {int((1-OUTLIER_QUANTILE)*100)}% wartości (limit 100 wypisanych):\")\n",
    "            shown = 0\n",
    "            for idx, col, val in removed_entries:\n",
    "                print(f\"  index={idx}  column='{col}'  value={val}\")\n",
    "                shown += 1\n",
    "                if shown >= 100:\n",
    "                    remaining = len(removed_entries) - shown\n",
    "                    if remaining > 0:\n",
    "                        print(f\"  ... oraz {remaining} pozostałych pozycji\")\n",
    "                    break\n",
    "            df = df.loc[~mask_total].copy()\n",
    "            print(f\"Usunięto {total_to_remove} wierszy.\")\n",
    "    \"\"\"\n",
    "\n",
    "    # 8. Reset indeksu — porządkujemy indeksy po ewentualnych usunięciach.\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Zwracamy: (wersja po czyszczeniu, snapshot przed outlierami)\n",
    "    return df, df_before_outlier_removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzy folder /output w którym będą zapisywane pliki CSV\n",
    "\n",
    "output_dir = Path(\"./output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# KOD DO CZYSZCZENIA DANYCH \n",
    "# Stosuje czyszczenie do wszystkich załadowanych df'ów\n",
    "cleaned_dataframes_dict: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "for name, df in dataframes_dict_dirty.items():\n",
    "    # 1) Wykonaj czyszczenie BEZ usuwania outlierów -> otrzymujemy (cleaned_no_outliers, snapshot_przed_outlierami)\n",
    "    cleaned_no_outliers, df_before_outliers = clean_df(df, name, remove_outliers=False)\n",
    "    \n",
    "    # 2) Wykonaj usunięcie outlierów na już wstępnie oczyszczonym df -> otrzymujemy (cleaned_with_outliers_removed, snapshot_przed_outlierami)\n",
    "    #    (druga wartość nie jest potrzebna tutaj, bo df_before_outliers już mamy)\n",
    "    # cleaned_with_outliers_removed, _ = clean_df(cleaned_no_outliers, name, remove_outliers=True)\n",
    "    \n",
    "    # Zapisz obie wersje do CSV (bez indeksu, UTF-8)\n",
    "    safe_name = name.replace(\" \", \"_\")\n",
    "    before_path = output_dir / f\"{safe_name}_before_outliers.csv\"\n",
    "    # after_path = output_dir / f\"{safe_name}_after_outliers.csv\"\n",
    "    \n",
    "    df_before_outliers.to_csv(before_path, index=False, encoding=\"utf-8\")\n",
    "    # cleaned_with_outliers_removed.to_csv(after_path, index=False, encoding=\"utf-8\")\n",
    "    \n",
    "    print(f\"Exported: {before_path.name} ({df_before_outliers.shape})\")\n",
    "    \n",
    "    # Zachowaj końcowy (po outlierach) w słowniku do dalszej analizy\n",
    "    cleaned_dataframes_dict[name] = df_before_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc3586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upewniamy się, że korzystamy ze świeżo wczytanych danych\n",
    "# (lista 'loaded' pochodzi z poprawionego skryptu wczytującego)\n",
    "dataframes_dict = dict(loaded)\n",
    "\n",
    "print(f\"Liczba poprawnych ramek danych do przetworzenia: {len(dataframes_dict)}\")\n",
    "\n",
    "# Tworzymy folder output\n",
    "output_dir = Path(\"./output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Słownik na oczyszczone dane\n",
    "cleaned_dataframes_dict = {}\n",
    "\n",
    "print(\"Rozpoczynam ponowny eksport na poprawnych danych...\")\n",
    "\n",
    "for name, df in dataframes_dict.items():\n",
    "    # Zabezpieczenie: jeśli mimo wszystko df jest pusty, pomiń go i zgłoś\n",
    "    if df.empty:\n",
    "        print(f\"⚠️ OSTRZEŻENIE: Ramka '{name}' jest pusta w pamięci! Pomijam.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # 1. Wykonaj czyszczenie (korzystamy z funkcji clean_df zdefiniowanej wcześniej)\n",
    "        # Zwraca: (wersja_czysta, wersja_przed_outlierami)\n",
    "        cleaned_final, df_snapshot = clean_df(df, name, remove_outliers=False)\n",
    "        \n",
    "        # 2. Generowanie nazwy pliku\n",
    "        safe_name = name.replace(\" \", \"_\")\n",
    "        before_path = output_dir / f\"{safe_name}_processed.csv\"\n",
    "        \n",
    "        # 3. ZAPIS (Format polski: średnik i przecinek)\n",
    "        df_snapshot.to_csv(\n",
    "            before_path, \n",
    "            index=False, \n",
    "            sep=';', \n",
    "            decimal=',', \n",
    "            encoding='utf-8-sig'\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Exported: {before_path.name:<45} | Shape: {df_snapshot.shape}\")\n",
    "        \n",
    "        # Zachowaj w pamięci wersję z kropkami (do obliczeń w Pythonie)\n",
    "        cleaned_dataframes_dict[name] = df_snapshot\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {name}: {e}\")\n",
    "\n",
    "print(f\"\\nZakończono. Pliki w: {output_dir.absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
