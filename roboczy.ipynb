{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d2179bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\nazar\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\nazar\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nazar\\appdata\\roaming\\python\\python314\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nazar\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nazar\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nazar\\appdata\\roaming\\python\\python314\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "import io\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283287a",
   "metadata": {},
   "source": [
    "Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7924976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam wczytywanie plików z walidacją kształtu danych...\n",
      "✅ Read graduates-institution.csv                | cp1250, sep=';', engine=c      | shape: (6876, 1538)\n",
      "✅ Read graduates-major-data.csv                 | utf-8, sep=';', engine=c       | shape: (70176, 438)\n",
      "✅ Read graduates-national.csv                   | utf-8, sep=';', engine=c       | shape: (30, 995)\n",
      "✅ Read international_rankings-institution.csv   | utf-8, sep=';', engine=c       | shape: (375, 151)\n",
      "✅ Read international_rankings-major.csv         | utf-8, sep=';', engine=c       | shape: (11186, 20)\n",
      "✅ Read students-institution.csv                 | utf-8, sep=';', engine=c       | shape: (5343, 1111)\n",
      "✅ Read students-major.csv                       | cp1250, sep=';', engine=c      | shape: (60428, 189)\n",
      "✅ Read students-national.csv                    | utf-8, sep=';', engine=c       | shape: (21, 568)\n",
      "✅ Read voivodeship-graduates-institution.csv    | utf-8, sep=';', engine=c       | shape: (3636, 491)\n",
      "✅ Read voivodeship-graduates-national.csv       | utf-8, sep=';', engine=c       | shape: (15, 7764)\n"
     ]
    }
   ],
   "source": [
    "# --- KONFIGURACJA ---\n",
    "data_dir = Path('./data')\n",
    "csv_paths = sorted(data_dir.glob('*.csv'))\n",
    "\n",
    "# Regex do filtrowania (bez zmian)\n",
    "mies_regex = re.compile(r'MIES', re.IGNORECASE)\n",
    "column_filter = lambda col: not mies_regex.search(col)\n",
    "\n",
    "# --- POPRAWIONA FUNKCJA WCZYTUJĄCA ---\n",
    "def try_read_csv(path, col_filter=None):\n",
    "    # ELA zazwyczaj używa utf-8 lub cp1250, a separatorem jest średnik.\n",
    "    # Zmieniam kolejność: najpierw średnik, potem reszta.\n",
    "    encodings = ['utf-8', 'utf-8-sig', 'cp1250', 'latin-1']\n",
    "    seps = [';', '\\t', ',', '|']\n",
    "    \n",
    "    read_kwargs = {'usecols': col_filter} if col_filter else {}\n",
    "    last_exc = None\n",
    "\n",
    "    for enc in encodings:\n",
    "        for sep in seps:\n",
    "            try:\n",
    "                # 1. Próba silnikiem C (szybki)\n",
    "                df = pd.read_csv(\n",
    "                    path, \n",
    "                    encoding=enc, \n",
    "                    sep=sep, \n",
    "                    engine='c', \n",
    "                    low_memory=False, \n",
    "                    on_bad_lines='skip',\n",
    "                    **read_kwargs\n",
    "                )\n",
    "                \n",
    "                # --- SANITY CHECK (NOWOŚĆ) ---\n",
    "                # Jeśli df jest pusty LUB ma mniej niż 2 kolumny -> to prawdopodobnie zły separator\n",
    "                if df.empty or df.shape[1] < 2:\n",
    "                    raise ValueError(f\"Suspicious shape {df.shape} with sep='{sep}'\")\n",
    "\n",
    "                return df, f\"{enc}, sep='{sep}', engine=c\"\n",
    "\n",
    "            except Exception as e_c:\n",
    "                # To nie jest właściwy separator/kodowanie, idziemy dalej\n",
    "                pass\n",
    "\n",
    "        # 2. Fallback: Silnik Python z autodetekcją (wolniejszy, inteligentniejszy)\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                path, \n",
    "                encoding=enc, \n",
    "                sep=None,  # Autodetekcja\n",
    "                engine='python', \n",
    "                on_bad_lines='skip',\n",
    "                **read_kwargs\n",
    "            )\n",
    "            \n",
    "            if df.empty or df.shape[1] < 2:\n",
    "                 raise ValueError(f\"Suspicious shape {df.shape} with auto-sep\")\n",
    "                 \n",
    "            return df, f\"{enc}, sep=auto, engine=python\"\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3. Ostatnia deska ratunku: wczytanie jako tekst i wymuszenie\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            raw = f.read()\n",
    "        \n",
    "        text = raw.decode('utf-8', errors='replace')\n",
    "        # Próba wymuszenia średnika na \"brudnym\" tekście\n",
    "        df = pd.read_csv(\n",
    "            io.StringIO(text), \n",
    "            sep=';', \n",
    "            engine='python', \n",
    "            on_bad_lines='skip',\n",
    "            **read_kwargs\n",
    "        )\n",
    "        return df, \"decoded bytes, forced sep=';'\"\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"CRITICAL: Could not read file. Last error: {e}\")\n",
    "\n",
    "# --- PĘTLA ŁADUJĄCA ---\n",
    "loaded = []\n",
    "print(\"Rozpoczynam wczytywanie plików z walidacją kształtu danych...\")\n",
    "\n",
    "for p in csv_paths:\n",
    "    try:\n",
    "        df, meta = try_read_csv(p, col_filter=column_filter)\n",
    "        print(f\"✅ Read {p.name:<40} | {meta:<30} | shape: {df.shape}\")\n",
    "        loaded.append((p.stem, df))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed {p.name}: {e}\")\n",
    "\n",
    "dataframes = [df for _, df in loaded]\n",
    "dataframes_dict = dict(loaded) # Zmieniam nazwę na czystszą"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a152727",
   "metadata": {},
   "source": [
    "Czyszczenie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1aaa8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykrywanie kolumn datowych\n",
    "def _looks_like_date_series(s: pd.Series, sample_size: int = 200, threshold: float = 0.5) -> bool:\n",
    "    \"\"\"\n",
    "    Sprawdza czy seria (kolumna) wygląda jak data.\n",
    "    Szuka wzorców takich jak: 2020-01-15, 01/12/2020, oraz nazw miesięcy\n",
    "    \"\"\"\n",
    "    # Konwertuje do tekstu i usuwa puste wartości\n",
    "    vals = s.dropna().astype(str)\n",
    "    if vals.empty:\n",
    "        return False\n",
    "    \n",
    "    # Bierze losową próbkę (max 200 wartości) do analizy\n",
    "    vals = vals.sample(min(len(vals), sample_size), random_state=0)\n",
    "    \n",
    "    # Regex dla dat: szuka liczb rozdzielonych myślnikami/kreskami/spacjami\n",
    "    # np. 2020-01-15 lub 15/12/2020\n",
    "    date_regex = re.compile(r'^\\s*\\d{1,4}(?:[\\-\\/\\.\\s]\\d{1,2}){1,2}\\s*$')\n",
    "    \n",
    "    # Regex dla nazw miesięcy (polska + angielska)\n",
    "    month_names = re.compile(\n",
    "        r'\\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|'\n",
    "        r'styc|lut|mar|kwi|maj|cze|lip|sie|wrz|paź|lis|gru)\\b', \n",
    "        re.I\n",
    "    )\n",
    "    \n",
    "    # Sprawdza czy wartości pasują do wzoru daty LUB zawierają nazwę miesiąca\n",
    "    matches = vals.str.match(date_regex) | vals.str.contains(month_names)\n",
    "    \n",
    "    # Jeśli co najmniej 50% wartości to daty, zwróć True\n",
    "    return matches.sum() >= max(1, int(threshold * len(vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "869a1914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Czyszczenie dataframe'u\n",
    "def clean_df(df: pd.DataFrame, name: str = \"\", remove_outliers: bool = True) -> tuple:\n",
    "    \"\"\"\n",
    "    Czyści ramkę danych i (opcjonalnie) usuwa outliery.\n",
    "    Zwraca: (df_po_czyszczeniu, df_przed_usunieciem_outlierow)\n",
    "    (Blok usuwania outlierów został zakomentowany.)\n",
    "    \"\"\"\n",
    "    # Robimy kopię, żeby nie modyfikować oryginalnego obiektu przekazanego do funkcji.\n",
    "    df = df.copy()\n",
    "\n",
    "    # Lista kolumn, które chcemy POMINAĆ przy wykrywaniu outlierów (np. ID, nazwy)\n",
    "    skip_columns_for_outliers = {\n",
    "        'p_rok_od', 'p_kierunek_id', 'p_poziom', 'p_forma', 'p_uczelnia_id',\n",
    "        'p_nazwa_uczelni', 'p_jedn', 'p_nazwa_jedn', 'p_woj', 'p_profil',\n",
    "        'p_dziedzina_new', 'p_uczelnia_skrot', 'p_poziom_tekst_pl',\n",
    "        'p_nazwa_kierunku_pelna', 'p_kierunek_nazwa', 'p_spec_nazwa', 'u_uczelnia_id', \"u_n\", \"u_n_wzus\",\"u_n_pozazus\", \"u_proc_wzus\",\"u_proc_pozazus\",\"u_n_dosw_rekr\", \"p_n\",\n",
    "        \"u_n_dosw_studia\", \"p_n_wzus\",\n",
    "    }\n",
    "\n",
    "    # KROK 1: Normalizuje nazwy kolumn\n",
    "    # Zamienia na małe litery, usuwa spacje, zastępuje je podkreśleniami\n",
    "    df.columns = [str(c).strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    # Usuwamy ewentualne duplikaty nazw kolumn (może się zdarzyć przy złym imporcie).\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # KROK 2: Usuwa całkowicie puste kolumny\n",
    "    # axis=1 oznacza kolumny, how=\"all\" oznacza całkowicie puste\n",
    "    df.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "    # KROK 3: Naprawia tekstowe kolumny\n",
    "    # Bierze tylko kolumny typu tekstowego (object)\n",
    "    obj_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "    for c in obj_cols:\n",
    "        # Konwertuj do string type, usuń spacje na początku/końcu, usuń znaki BOM\n",
    "        df[c] = df[c].astype(\"string\").str.strip().str.replace(\"\\ufeff\", \"\", regex=False)\n",
    "\n",
    "         # Zamienia typowe znaczniki braku danych na pd.NA (None)\n",
    "    df.replace({\"\": pd.NA, \"NA\": pd.NA, \"N/A\": pd.NA, \"na\": pd.NA, \"-\": pd.NA, \"—\": pd.NA, \"None\": pd.NA}, inplace=True)\n",
    "\n",
    "    # KROK 4: Konwertuje kolumny liczbowe (tekst → liczby)\n",
    "    for c in df.columns:\n",
    "        # Jeśli kolumna jest tekstem, spróbuj skonwertować na liczby\n",
    "        if df[c].dtype == \"object\" or pd.api.types.is_string_dtype(df[c]):\n",
    "            # Usuń spacje wewnątrz liczb\n",
    "            s = df[c].astype(\"string\").str.replace(r\"\\s+\", \"\", regex=True)\n",
    "            # Zamień przecinki na kropki (polski format → międzynarodowy)\n",
    "            s = s.str.replace(\",\", \".\", regex=False)\n",
    "            # Usuń wszystkie znaki oprócz cyfr, kropki i minusa\n",
    "            s_clean = s.str.replace(r\"[^0-9\\.\\-]\", \"\", regex=True)\n",
    "            # Konwertuj na liczby (błędy zamień na NaN)\n",
    "            coerced = pd.to_numeric(s_clean, errors=\"coerce\")\n",
    "            non_null_count = coerced.notna().sum()\n",
    "            # Jeśli co najmniej 30% wartości to liczby (nie puste), zaakceptuj konwersję\n",
    "            if non_null_count > 0 and non_null_count >= max(1, int(0.3 * len(coerced))):\n",
    "                df[c] = coerced\n",
    "\n",
    "    # KROK 5: Konwertuje kolumny datowe (tekst → daty)\n",
    "    for c in df.columns:\n",
    "        # Jeśli kolumna jest tekstem\n",
    "        if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_string_dtype(df[c]):\n",
    "            # Sprawdź czy wygląda jak data\n",
    "            if _looks_like_date_series(df[c]):\n",
    "                # Wyłącz warningi podczas konwersji\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "                    # Konwertuj na datetime, przyjmując format DD/MM/YYYY\n",
    "                    parsed = pd.to_datetime(df[c], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "                    # Jeśli co najmniej 30% wartości zostało skonwertowane, zaakceptuj\n",
    "                if parsed.notna().sum() >= max(1, int(0.3 * len(parsed))):\n",
    "                    df[c] = parsed\n",
    "\n",
    "    # KROK 6: Usuwa puste wiersze i duplikaty\n",
    "    # Usuwa wiersze które są całkowicie puste\n",
    "    df.dropna(axis=0, how=\"all\", inplace=True)\n",
    "    # Usuwa wiersze które są całkowicie identyczne\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Zapisujemy snapshot ramki po standardowym czyszczeniu, przed ewentualnym usuwaniem outlierów.\n",
    "    df_before_outlier_removal = df.copy()\n",
    "\n",
    "    # KROK 7: Usuwa wartości skrajne (outliers) — metoda IQR\n",
    "    # POMIJAMY WYBRANE KOLUMNY (takie jak ID, kody, poziomy, kierunki itd.)\n",
    "    \"\"\"\n",
    "    if remove_outliers:\n",
    "        OUTLIER_QUANTILE = 0.99\n",
    "        numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns.tolist()]\n",
    "\n",
    "        mask_total = pd.Series(False, index=df.index)\n",
    "        removed_entries = []\n",
    "\n",
    "        for c in numeric_cols:\n",
    "            if c in skip_columns_for_outliers:\n",
    "                continue\n",
    "            upper = df[c].quantile(OUTLIER_QUANTILE)\n",
    "            if pd.isna(upper):\n",
    "                continue\n",
    "            mask = df[c].notna() & (df[c] > upper)\n",
    "            if mask.any():\n",
    "                for idx in df.index[mask]:\n",
    "                    removed_entries.append((idx, c, df.at[idx, c]))\n",
    "                mask_total |= mask\n",
    "\n",
    "        if mask_total.any():\n",
    "            total_to_remove = int(mask_total.sum())\n",
    "            print(f\"[outliers - percentyl {OUTLIER_QUANTILE}] {name} :: usuwam {total_to_remove} wierszy zawierających TOP {int((1-OUTLIER_QUANTILE)*100)}% wartości (limit 100 wypisanych):\")\n",
    "            shown = 0\n",
    "            for idx, col, val in removed_entries:\n",
    "                print(f\"  index={idx}  column='{col}'  value={val}\")\n",
    "                shown += 1\n",
    "                if shown >= 100:\n",
    "                    remaining = len(removed_entries) - shown\n",
    "                    if remaining > 0:\n",
    "                        print(f\"  ... oraz {remaining} pozostałych pozycji\")\n",
    "                    break\n",
    "            df = df.loc[~mask_total].copy()\n",
    "            print(f\"Usunięto {total_to_remove} wierszy.\")\n",
    "    \"\"\"\n",
    "\n",
    "    # 8. Reset indeksu — porządkujemy indeksy po ewentualnych usunięciach.\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Zwracamy: (wersja po czyszczeniu, snapshot przed outlierami)\n",
    "    return df, df_before_outlier_removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "601e6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: graduates-institution_before_outliers.csv ((0, 0))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m cleaned_dataframes_dict: Dict[\u001b[38;5;28mstr\u001b[39m, pd.DataFrame] = {}\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, df \u001b[38;5;129;01min\u001b[39;00m dataframes_dict_dirty.items():\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# 1) Wykonaj czyszczenie BEZ usuwania outlierów -> otrzymujemy (cleaned_no_outliers, snapshot_przed_outlierami)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     cleaned_no_outliers, df_before_outliers = \u001b[43mclean_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_outliers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# 2) Wykonaj usunięcie outlierów na już wstępnie oczyszczonym df -> otrzymujemy (cleaned_with_outliers_removed, snapshot_przed_outlierami)\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m#    (druga wartość nie jest potrzebna tutaj, bo df_before_outliers już mamy)\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# cleaned_with_outliers_removed, _ = clean_df(cleaned_no_outliers, name, remove_outliers=True)\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Zapisz obie wersje do CSV (bez indeksu, UTF-8)\u001b[39;00m\n\u001b[32m     19\u001b[39m     safe_name = name.replace(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mclean_df\u001b[39m\u001b[34m(df, name, remove_outliers)\u001b[39m\n\u001b[32m     35\u001b[39m     df[c] = df[c].astype(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m).str.strip().str.replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\ufeff\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, regex=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     37\u001b[39m      \u001b[38;5;66;03m# Zamienia typowe znaczniki braku danych na pd.NA (None)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mN/A\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mna\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m—\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNA\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# KROK 4: Konwertuje kolumny liczbowe (tekst → liczby)\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# Jeśli kolumna jest tekstem, spróbuj skonwertować na liczby\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nazar\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\generic.py:8077\u001b[39m, in \u001b[36mNDFrame.replace\u001b[39m\u001b[34m(self, to_replace, value, inplace, limit, regex, method)\u001b[39m\n\u001b[32m   8074\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   8075\u001b[39m         to_replace, value = keys, values\n\u001b[32m-> \u001b[39m\u001b[32m8077\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   8078\u001b[39m \u001b[43m        \u001b[49m\u001b[43mto_replace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregex\u001b[49m\n\u001b[32m   8079\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   8080\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   8081\u001b[39m     \u001b[38;5;66;03m# need a non-zero len on all axes\u001b[39;00m\n\u001b[32m   8082\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.size:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nazar\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\generic.py:8125\u001b[39m, in \u001b[36mNDFrame.replace\u001b[39m\u001b[34m(self, to_replace, value, inplace, limit, regex, method)\u001b[39m\n\u001b[32m   8120\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(to_replace) != \u001b[38;5;28mlen\u001b[39m(value):\n\u001b[32m   8121\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   8122\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReplacement lists must match in length. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   8123\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(to_replace)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   8124\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m8125\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   8126\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto_replace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdest_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8128\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8130\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   8132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m to_replace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   8133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[32m   8134\u001b[39m         is_re_compilable(regex)\n\u001b[32m   8135\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m is_list_like(regex)\n\u001b[32m   8136\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m is_dict_like(regex)\n\u001b[32m   8137\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nazar\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\internals\\base.py:278\u001b[39m, in \u001b[36mDataManager.replace_list\u001b[39m\u001b[34m(self, src_list, dest_list, inplace, regex)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"do a list replace\"\"\"\u001b[39;00m\n\u001b[32m    276\u001b[39m inplace = validate_bool_kwarg(inplace, \u001b[33m\"\u001b[39m\u001b[33minplace\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m bm = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_with_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreplace_list\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdest_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdest_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43malready_warned\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_AlreadyWarned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m bm._consolidate_inplace()\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bm\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nazar\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nazar\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1136\u001b[39m, in \u001b[36mBlock.replace_list\u001b[39m\u001b[34m(self, src_list, dest_list, inplace, regex, using_cow, already_warned)\u001b[39m\n\u001b[32m   1133\u001b[39m \u001b[38;5;66;03m# Materialize if inplace = True, since the masks can change\u001b[39;00m\n\u001b[32m   1134\u001b[39m \u001b[38;5;66;03m# as we replace\u001b[39;00m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     masks = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m using_cow:\n\u001b[32m   1139\u001b[39m     \u001b[38;5;66;03m# Don't set up refs here, otherwise we will think that we have\u001b[39;00m\n\u001b[32m   1140\u001b[39m     \u001b[38;5;66;03m# references when we check again later\u001b[39;00m\n\u001b[32m   1141\u001b[39m     rb = [\u001b[38;5;28mself\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nazar\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1123\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_string_dtype(values.dtype):\n\u001b[32m   1116\u001b[39m     \u001b[38;5;66;03m# Calculate the mask once, prior to the call of comp\u001b[39;00m\n\u001b[32m   1117\u001b[39m     \u001b[38;5;66;03m# in order to avoid repeating the same computations\u001b[39;00m\n\u001b[32m   1118\u001b[39m     na_mask = ~isna(values)\n\u001b[32m   1119\u001b[39m     masks: Iterable[npt.NDArray[np.bool_]] = (\n\u001b[32m   1120\u001b[39m         extract_bool_array(\n\u001b[32m   1121\u001b[39m             cast(\n\u001b[32m   1122\u001b[39m                 ArrayLike,\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m                 \u001b[43mcompare_or_regex_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_mask\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1126\u001b[39m             )\n\u001b[32m   1127\u001b[39m         )\n\u001b[32m   1128\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m pairs\n\u001b[32m   1129\u001b[39m     )\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1131\u001b[39m     \u001b[38;5;66;03m# GH#38086 faster if we know we dont need to check for regex\u001b[39;00m\n\u001b[32m   1132\u001b[39m     masks = (missing.mask_missing(values, s[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m pairs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nazar\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\array_algos\\replace.py:106\u001b[39m, in \u001b[36mcompare_or_regex_search\u001b[39m\u001b[34m(a, b, regex, mask)\u001b[39m\n\u001b[32m    104\u001b[39m         result = tmp\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     result = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m _check_comparison_types(result, a, b)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nazar\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\array_algos\\replace.py:86\u001b[39m, in \u001b[36mcompare_or_regex_search.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m     81\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot compare types \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(type_names[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(type_names[\u001b[32m1\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     82\u001b[39m         )\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m regex \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_use_regex(regex, b):\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# TODO: should use missing.mask_missing?\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     op = \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43meq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     88\u001b[39m     op = np.vectorize(\n\u001b[32m     89\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mbool\u001b[39m(re.search(b, x))\n\u001b[32m     90\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, (\u001b[38;5;28mstr\u001b[39m, Pattern))\n\u001b[32m     91\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     92\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nazar\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nazar\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\arraylike.py:40\u001b[39m, in \u001b[36mOpsMixin.__eq__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__eq__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nazar\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\arrays\\string_.py:1077\u001b[39m, in \u001b[36mStringArray._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   1074\u001b[39m mask = isna(\u001b[38;5;28mself\u001b[39m) | isna(other)\n\u001b[32m   1075\u001b[39m valid = ~mask\n\u001b[32m-> \u001b[39m\u001b[32m1077\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_list_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1078\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(other) != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1079\u001b[39m         \u001b[38;5;66;03m# prevent improper broadcasting when other is 2D\u001b[39;00m\n\u001b[32m   1080\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1081\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLengths of operands do not match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(other)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1082\u001b[39m         )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Tworzy folder /output w którym będą zapisywane pliki CSV\n",
    "\n",
    "output_dir = Path(\"./output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# KOD DO CZYSZCZENIA DANYCH \n",
    "# Stosuje czyszczenie do wszystkich załadowanych df'ów\n",
    "cleaned_dataframes_dict: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "for name, df in dataframes_dict_dirty.items():\n",
    "    # 1) Wykonaj czyszczenie BEZ usuwania outlierów -> otrzymujemy (cleaned_no_outliers, snapshot_przed_outlierami)\n",
    "    cleaned_no_outliers, df_before_outliers = clean_df(df, name, remove_outliers=False)\n",
    "    \n",
    "    # 2) Wykonaj usunięcie outlierów na już wstępnie oczyszczonym df -> otrzymujemy (cleaned_with_outliers_removed, snapshot_przed_outlierami)\n",
    "    #    (druga wartość nie jest potrzebna tutaj, bo df_before_outliers już mamy)\n",
    "    # cleaned_with_outliers_removed, _ = clean_df(cleaned_no_outliers, name, remove_outliers=True)\n",
    "    \n",
    "    # Zapisz obie wersje do CSV (bez indeksu, UTF-8)\n",
    "    safe_name = name.replace(\" \", \"_\")\n",
    "    before_path = output_dir / f\"{safe_name}_before_outliers.csv\"\n",
    "    # after_path = output_dir / f\"{safe_name}_after_outliers.csv\"\n",
    "    \n",
    "    df_before_outliers.to_csv(before_path, index=False, encoding=\"utf-8\")\n",
    "    # cleaned_with_outliers_removed.to_csv(after_path, index=False, encoding=\"utf-8\")\n",
    "    \n",
    "    print(f\"Exported: {before_path.name} ({df_before_outliers.shape})\")\n",
    "    \n",
    "    # Zachowaj końcowy (po outlierach) w słowniku do dalszej analizy\n",
    "    cleaned_dataframes_dict[name] = df_before_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aebc3586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba poprawnych ramek danych do przetworzenia: 10\n",
      "Rozpoczynam ponowny eksport na poprawnych danych...\n",
      "✅ Exported: graduates-institution_processed.csv           | Shape: (6876, 1538)\n",
      "✅ Exported: graduates-major-data_processed.csv            | Shape: (70176, 438)\n",
      "✅ Exported: graduates-national_processed.csv              | Shape: (30, 995)\n",
      "✅ Exported: international_rankings-institution_processed.csv | Shape: (375, 151)\n",
      "✅ Exported: international_rankings-major_processed.csv    | Shape: (11186, 20)\n",
      "✅ Exported: students-institution_processed.csv            | Shape: (5343, 1111)\n",
      "✅ Exported: students-major_processed.csv                  | Shape: (60428, 189)\n",
      "✅ Exported: students-national_processed.csv               | Shape: (21, 568)\n",
      "✅ Exported: voivodeship-graduates-institution_processed.csv | Shape: (3636, 491)\n",
      "✅ Exported: voivodeship-graduates-national_processed.csv  | Shape: (15, 7764)\n",
      "\n",
      "Zakończono. Pliki w: c:\\Users\\Nazar\\Documents\\Analiza Danych z R i Pythonem\\Analiza-danych-w-R-i-Python\\output\n"
     ]
    }
   ],
   "source": [
    "# Upewniamy się, że korzystamy ze świeżo wczytanych danych\n",
    "# (lista 'loaded' pochodzi z poprawionego skryptu wczytującego)\n",
    "dataframes_dict = dict(loaded)\n",
    "\n",
    "print(f\"Liczba poprawnych ramek danych do przetworzenia: {len(dataframes_dict)}\")\n",
    "\n",
    "# Tworzymy folder output\n",
    "output_dir = Path(\"./output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Słownik na oczyszczone dane\n",
    "cleaned_dataframes_dict = {}\n",
    "\n",
    "print(\"Rozpoczynam ponowny eksport na poprawnych danych...\")\n",
    "\n",
    "for name, df in dataframes_dict.items():\n",
    "    # Zabezpieczenie: jeśli mimo wszystko df jest pusty, pomiń go i zgłoś\n",
    "    if df.empty:\n",
    "        print(f\"⚠️ OSTRZEŻENIE: Ramka '{name}' jest pusta w pamięci! Pomijam.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # 1. Wykonaj czyszczenie (korzystamy z funkcji clean_df zdefiniowanej wcześniej)\n",
    "        # Zwraca: (wersja_czysta, wersja_przed_outlierami)\n",
    "        cleaned_final, df_snapshot = clean_df(df, name, remove_outliers=False)\n",
    "        \n",
    "        # 2. Generowanie nazwy pliku\n",
    "        safe_name = name.replace(\" \", \"_\")\n",
    "        before_path = output_dir / f\"{safe_name}_processed.csv\"\n",
    "        \n",
    "        # 3. ZAPIS (Format polski: średnik i przecinek)\n",
    "        df_snapshot.to_csv(\n",
    "            before_path, \n",
    "            index=False, \n",
    "            sep=';', \n",
    "            decimal=',', \n",
    "            encoding='utf-8-sig'\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Exported: {before_path.name:<45} | Shape: {df_snapshot.shape}\")\n",
    "        \n",
    "        # Zachowaj w pamięci wersję z kropkami (do obliczeń w Pythonie)\n",
    "        cleaned_dataframes_dict[name] = df_snapshot\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {name}: {e}\")\n",
    "\n",
    "print(f\"\\nZakończono. Pliki w: {output_dir.absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
