{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2179bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "import io\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283287a",
   "metadata": {},
   "source": [
    "Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzam czy sciezka data istnieje\n",
    "data_dir = Path('./data')\n",
    "if not data_dir.exists():\n",
    "    raise FileNotFoundError(f\"{data_dir} does not exist\")\n",
    "\n",
    "# Pobieram wszystkie pliki z rozszerzeniem CSV\n",
    "csv_paths = sorted(data_dir.glob('*.csv'))\n",
    "\n",
    "# Wczytuje pliki CSV z różnymi kodowaniami i separatorami\n",
    "def try_read_csv(path):\n",
    "    encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1250', 'cp1252']\n",
    "    seps = [';', ',', '\\t', '|']\n",
    "    last_exc = None\n",
    "\n",
    "    # Dla znanych separatorów preferuj engine='c' z low_memory=False (szybszy),\n",
    "    # a jeśli się nie uda — engine='python' (bez low_memory).\n",
    "    for enc in encodings:\n",
    "        for sep in seps:\n",
    "            try:\n",
    "                df = pd.read_csv(path, encoding=enc, sep=sep, engine='c', low_memory=False, on_bad_lines='skip')\n",
    "                return df, f\"{enc}, sep='{sep}', engine=c\"\n",
    "            except Exception as e_c:\n",
    "                last_exc = e_c\n",
    "                try:\n",
    "                    df = pd.read_csv(path, encoding=enc, sep=sep, engine='python', on_bad_lines='skip')\n",
    "                    return df, f\"{enc}, sep='{sep}', engine=python\"\n",
    "                except Exception as e_py:\n",
    "                    last_exc = e_py\n",
    "\n",
    "    # Sprawdzam encodings (typy zakodowań tekstu)(dodane z powodu warningów)\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=enc, sep=None, engine='python', on_bad_lines='skip')\n",
    "            return df, f\"{enc}, sep=auto, engine=python\"\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "\n",
    "    # Warningi - niektóry tekst się nie wgrywał - dekodujemy ręcznie i czytamy jako utf-8 \n",
    "    try:\n",
    "        raw = path.read_bytes()\n",
    "        text = None\n",
    "        for enc in encodings:\n",
    "            try:\n",
    "                text = raw.decode(enc)\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        if text is None:\n",
    "            text = raw.decode('utf-8', errors='replace')\n",
    "        df = pd.read_csv(io.StringIO(text), sep=None, engine='python', on_bad_lines='skip')\n",
    "        return df, \"decoded with replace, sep=auto, engine=python\"\n",
    "    except Exception as e:\n",
    "        raise last_exc or e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b71e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytuję wszystkie pliki CSV (wywołując funkcję powyżej)\n",
    "# lista zawiera wszystkie df'y\n",
    "loaded = []\n",
    "for p in csv_paths:\n",
    "    try:\n",
    "        df, meta = try_read_csv(p)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {p.name}: {e}\")\n",
    "    else:\n",
    "        print(f\"Read {p.name} ({meta}) — shape: {df.shape}\")\n",
    "        loaded.append((p.stem, df))\n",
    "\n",
    "dataframes = [df for _, df in loaded]\n",
    "dataframes_dict_dirty = dict(loaded)\n",
    "print(f\"Loaded {len(dataframes)} CSV files: {[name for name, _ in loaded]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a152727",
   "metadata": {},
   "source": [
    "Czyszczenie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykrywanie kolumn datowych\n",
    "def _looks_like_date_series(s: pd.Series, sample_size: int = 200, threshold: float = 0.5) -> bool:\n",
    "    \"\"\"\n",
    "    Sprawdza czy seria (kolumna) wygląda jak data.\n",
    "    Szuka wzorców takich jak: 2020-01-15, 01/12/2020, oraz nazw miesięcy\n",
    "    \"\"\"\n",
    "    # Konwertuje do tekstu i usuwa puste wartości\n",
    "    vals = s.dropna().astype(str)\n",
    "    if vals.empty:\n",
    "        return False\n",
    "    \n",
    "    # Bierze losową próbkę (max 200 wartości) do analizy\n",
    "    vals = vals.sample(min(len(vals), sample_size), random_state=0)\n",
    "    \n",
    "    # Regex dla dat: szuka liczb rozdzielonych myślnikami/kreskami/spacjami\n",
    "    # np. 2020-01-15 lub 15/12/2020\n",
    "    date_regex = re.compile(r'^\\s*\\d{1,4}(?:[\\-\\/\\.\\s]\\d{1,2}){1,2}\\s*$')\n",
    "    \n",
    "    # Regex dla nazw miesięcy (polska + angielska)\n",
    "    month_names = re.compile(\n",
    "        r'\\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|'\n",
    "        r'styc|lut|mar|kwi|maj|cze|lip|sie|wrz|paź|lis|gru)\\b', \n",
    "        re.I\n",
    "    )\n",
    "    \n",
    "    # Sprawdza czy wartości pasują do wzoru daty LUB zawierają nazwę miesiąca\n",
    "    matches = vals.str.match(date_regex) | vals.str.contains(month_names)\n",
    "    \n",
    "    # Jeśli co najmniej 50% wartości to daty, zwróć True\n",
    "    return matches.sum() >= max(1, int(threshold * len(vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a1914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Czyszczenie dataframe'u\n",
    "def clean_df(df: pd.DataFrame, name: str = \"\", remove_outliers: bool = True) -> tuple:\n",
    "    \"\"\"\n",
    "    Czyści ramkę danych i (opcjonalnie) usuwa outliery.\n",
    "    Zwraca: (df_po_czyszczeniu, df_przed_usunieciem_outlierow)\n",
    "    (Blok usuwania outlierów został zakomentowany.)\n",
    "    \"\"\"\n",
    "    # Robimy kopię, żeby nie modyfikować oryginalnego obiektu przekazanego do funkcji.\n",
    "    df = df.copy()\n",
    "\n",
    "    # Lista kolumn, które chcemy POMINAĆ przy wykrywaniu outlierów (np. ID, nazwy)\n",
    "    skip_columns_for_outliers = {\n",
    "        'p_rok_od', 'p_kierunek_id', 'p_poziom', 'p_forma', 'p_uczelnia_id',\n",
    "        'p_nazwa_uczelni', 'p_jedn', 'p_nazwa_jedn', 'p_woj', 'p_profil',\n",
    "        'p_dziedzina_new', 'p_uczelnia_skrot', 'p_poziom_tekst_pl',\n",
    "        'p_nazwa_kierunku_pelna', 'p_kierunek_nazwa', 'p_spec_nazwa', 'u_uczelnia_id', \"u_n\", \"u_n_wzus\",\"u_n_pozazus\", \"u_proc_wzus\",\"u_proc_pozazus\",\"u_n_dosw_rekr\", \"p_n\",\n",
    "        \"u_n_dosw_studia\", \"p_n_wzus\",\n",
    "    }\n",
    "\n",
    "    # KROK 1: Normalizuje nazwy kolumn\n",
    "    # Zamienia na małe litery, usuwa spacje, zastępuje je podkreśleniami\n",
    "    df.columns = [str(c).strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    # Usuwamy ewentualne duplikaty nazw kolumn (może się zdarzyć przy złym imporcie).\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # KROK 2: Usuwa całkowicie puste kolumny\n",
    "    # axis=1 oznacza kolumny, how=\"all\" oznacza całkowicie puste\n",
    "    df.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "    # KROK 3: Naprawia tekstowe kolumny\n",
    "    # Bierze tylko kolumny typu tekstowego (object)\n",
    "    obj_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "    for c in obj_cols:\n",
    "        # Konwertuj do string type, usuń spacje na początku/końcu, usuń znaki BOM\n",
    "        df[c] = df[c].astype(\"string\").str.strip().str.replace(\"\\ufeff\", \"\", regex=False)\n",
    "\n",
    "         # Zamienia typowe znaczniki braku danych na pd.NA (None)\n",
    "    df.replace({\"\": pd.NA, \"NA\": pd.NA, \"N/A\": pd.NA, \"na\": pd.NA, \"-\": pd.NA, \"—\": pd.NA, \"None\": pd.NA}, inplace=True)\n",
    "\n",
    "    # KROK 4: Konwertuje kolumny liczbowe (tekst → liczby)\n",
    "    for c in df.columns:\n",
    "        # Jeśli kolumna jest tekstem, spróbuj skonwertować na liczby\n",
    "        if df[c].dtype == \"object\" or pd.api.types.is_string_dtype(df[c]):\n",
    "            # Usuń spacje wewnątrz liczb\n",
    "            s = df[c].astype(\"string\").str.replace(r\"\\s+\", \"\", regex=True)\n",
    "            # Zamień przecinki na kropki (polski format → międzynarodowy)\n",
    "            s = s.str.replace(\",\", \".\", regex=False)\n",
    "            # Usuń wszystkie znaki oprócz cyfr, kropki i minusa\n",
    "            s_clean = s.str.replace(r\"[^0-9\\.\\-]\", \"\", regex=True)\n",
    "            # Konwertuj na liczby (błędy zamień na NaN)\n",
    "            coerced = pd.to_numeric(s_clean, errors=\"coerce\")\n",
    "            non_null_count = coerced.notna().sum()\n",
    "            # Jeśli co najmniej 30% wartości to liczby (nie puste), zaakceptuj konwersję\n",
    "            if non_null_count > 0 and non_null_count >= max(1, int(0.3 * len(coerced))):\n",
    "                df[c] = coerced\n",
    "\n",
    "    # KROK 5: Konwertuje kolumny datowe (tekst → daty)\n",
    "    for c in df.columns:\n",
    "        # Jeśli kolumna jest tekstem\n",
    "        if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_string_dtype(df[c]):\n",
    "            # Sprawdź czy wygląda jak data\n",
    "            if _looks_like_date_series(df[c]):\n",
    "                # Wyłącz warningi podczas konwersji\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "                    # Konwertuj na datetime, przyjmując format DD/MM/YYYY\n",
    "                    parsed = pd.to_datetime(df[c], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "                    # Jeśli co najmniej 30% wartości zostało skonwertowane, zaakceptuj\n",
    "                if parsed.notna().sum() >= max(1, int(0.3 * len(parsed))):\n",
    "                    df[c] = parsed\n",
    "\n",
    "    # KROK 6: Usuwa puste wiersze i duplikaty\n",
    "    # Usuwa wiersze które są całkowicie puste\n",
    "    df.dropna(axis=0, how=\"all\", inplace=True)\n",
    "    # Usuwa wiersze które są całkowicie identyczne\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Zapisujemy snapshot ramki po standardowym czyszczeniu, przed ewentualnym usuwaniem outlierów.\n",
    "    df_before_outlier_removal = df.copy()\n",
    "\n",
    "    # KROK 7: Usuwa wartości skrajne (outliers) — metoda IQR\n",
    "    # POMIJAMY WYBRANE KOLUMNY (takie jak ID, kody, poziomy, kierunki itd.)\n",
    "    \"\"\"\n",
    "    if remove_outliers:\n",
    "        OUTLIER_QUANTILE = 0.99\n",
    "        numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns.tolist()]\n",
    "\n",
    "        mask_total = pd.Series(False, index=df.index)\n",
    "        removed_entries = []\n",
    "\n",
    "        for c in numeric_cols:\n",
    "            if c in skip_columns_for_outliers:\n",
    "                continue\n",
    "            upper = df[c].quantile(OUTLIER_QUANTILE)\n",
    "            if pd.isna(upper):\n",
    "                continue\n",
    "            mask = df[c].notna() & (df[c] > upper)\n",
    "            if mask.any():\n",
    "                for idx in df.index[mask]:\n",
    "                    removed_entries.append((idx, c, df.at[idx, c]))\n",
    "                mask_total |= mask\n",
    "\n",
    "        if mask_total.any():\n",
    "            total_to_remove = int(mask_total.sum())\n",
    "            print(f\"[outliers - percentyl {OUTLIER_QUANTILE}] {name} :: usuwam {total_to_remove} wierszy zawierających TOP {int((1-OUTLIER_QUANTILE)*100)}% wartości (limit 100 wypisanych):\")\n",
    "            shown = 0\n",
    "            for idx, col, val in removed_entries:\n",
    "                print(f\"  index={idx}  column='{col}'  value={val}\")\n",
    "                shown += 1\n",
    "                if shown >= 100:\n",
    "                    remaining = len(removed_entries) - shown\n",
    "                    if remaining > 0:\n",
    "                        print(f\"  ... oraz {remaining} pozostałych pozycji\")\n",
    "                    break\n",
    "            df = df.loc[~mask_total].copy()\n",
    "            print(f\"Usunięto {total_to_remove} wierszy.\")\n",
    "    \"\"\"\n",
    "\n",
    "    # 8. Reset indeksu — porządkujemy indeksy po ewentualnych usunięciach.\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Zwracamy: (wersja po czyszczeniu, snapshot przed outlierami)\n",
    "    return df, df_before_outlier_removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DEFINICJA KOLUMN (Oszczędzamy RAM, wczytujemy tylko to co niezbędne)\n",
    "# Zawsze korzystamy ze słownika zmiennych!\n",
    "cols_to_load = [\n",
    "    'P_DZIEDZINA', \n",
    "    'P_WWZ_P1', 'P_WWZ_P2', 'P_WWZ_P3', 'P_WWZ_P4', 'P_WWZ_P5',\n",
    "    'P_WWB_P1', 'P_WWB_P2', 'P_WWB_P3', 'P_WWB_P4', 'P_WWB_P5'\n",
    "]\n",
    "\n",
    "# Wczytanie danych (zakładam, że plik z danymi to 'graduates-major-data.csv')\n",
    "# Pamiętaj o odpowiednim kodowaniu i separatorze (często w ELA to ';')\n",
    "df = pd.read_csv('graduates-major-data.csv', usecols=cols_to_load, sep=',') \n",
    "\n",
    "# 2. OBLICZENIE INDEKSU SYNTETYCZNEGO (Dla każdego roku osobno)\n",
    "# Logika: Wynik = Zarobki (im więcej tym lepiej) - Ryzyko Bezrobocia (im mniej tym lepiej)\n",
    "for i in range(1, 6):\n",
    "    df[f'Score_P{i}'] = df[f'P_WWZ_P{i}'] - df[f'P_WWB_P{i}']\n",
    "\n",
    "# 3. AGREGACJA (Grupowanie po Dziedzinie)\n",
    "# Liczymy średni wynik dla całej dziedziny (np. Nauki Techniczne vs Humanistyczne)\n",
    "df_grouped = df.groupby('P_DZIEDZINA')[[f'Score_P{i}' for i in range(1, 6)]].mean().reset_index()\n",
    "\n",
    "# 4. TRANSFORMACJA WIDE TO LONG (Kluczowe dla wykresów liniowych!)\n",
    "# Zmieniamy tabelę z szerokiej (kolumny lat obok siebie) na długą (jeden wiersz = jedna obserwacja rok-dziedzina)\n",
    "df_melted = df_grouped.melt(\n",
    "    id_vars=['P_DZIEDZINA'], \n",
    "    var_name='Okres', \n",
    "    value_name='Syntetyczny_Wskaznik'\n",
    ")\n",
    "\n",
    "# Czyszczenie kolumny 'Okres', żeby mieć liczby 1-5 zamiast stringów 'Score_P1'\n",
    "df_melted['Rok_po_dyplomie'] = df_melted['Okres'].str.extract('(\\d)').astype(int)\n",
    "\n",
    "# 5. WIZUALIZACJA (Storytelling)\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "sns.lineplot(\n",
    "    data=df_melted, \n",
    "    x='Rok_po_dyplomie', \n",
    "    y='Syntetyczny_Wskaznik', \n",
    "    hue='P_DZIEDZINA',    # Każda dziedzina inny kolor\n",
    "    marker='o',           # Kropki na punktach\n",
    "    linewidth=2.5\n",
    ")\n",
    "\n",
    "plt.title('Syntetyczny Wskaźnik Sukcesu Zawodowego (WWZ - WWB) w czasie', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Lata po uzyskaniu dyplomu', fontsize=12)\n",
    "plt.ylabel('Wynik (Wyższy = Lepszy)', fontsize=12)\n",
    "plt.axhline(0, color='black', linestyle='--', alpha=0.5) # Linia odniesienia (Zero)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Dziedzina')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
